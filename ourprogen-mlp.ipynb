{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13276026,"sourceType":"datasetVersion","datasetId":8413353},{"sourceId":13276088,"sourceType":"datasetVersion","datasetId":8413401}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nAdapted classifier using custom ProteinLM + advanced MLP with augmentation, PCA, and weighted sampling.\nAchieves high accuracy by incorporating data augmentation, class balancing, and optimized training.\n\"\"\"\n\nimport os\nimport time\nimport random\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset, WeightedRandomSampler\nfrom torch.optim import AdamW\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nimport joblib  # for saving label encoder & PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# -----------------------------\n# CONFIGURATION (from high-accuracy setup)\n# -----------------------------\nDATA_PATH = \"/kaggle/input/datasetmlpclass/shuffled_standardized_add.xlsx\"  # Adjusted for xlsx\nSEQ_COL = \"SubSequence\"\nLABEL_COL = \"Clinical_Significance\"\nMAX_TOKEN_LENGTH = 512\nBATCH_EMBED = 8\nPCA_DIM = 512\nTEST_SIZE = 0.2\nRANDOM_SEED = 42\n\nUSE_AUGMENT = True\nAUG_SHIFTS = [-2, 0, 2]\nWINDOW_LEN = 50\n\nHIDDEN_DIM_1 = 512\nHIDDEN_DIM_2 = 256\nDROPOUT = 0.4\nLR = 1e-4\nWEIGHT_DECAY = 1e-3\nBATCH_TRAIN = 32\nNUM_EPOCHS = 100\nPATIENCE = 25\nLR_PLATEAU_FACTOR = 0.5\nLR_PLATEAU_PATIENCE = 5\n\nUSE_SAMPLER = True\n\n# Custom ProteinLM checkpoint\nCKPT_PATH = \"/kaggle/input/progenmodel/final_ckpt.pt\"\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", DEVICE)\n\n# -----------------------------\n# 1) Tokenizer / Vocabulary (Custom)\n# -----------------------------\nAMINO_ACIDS = list(\"ACDEFGHIKLMNPQRSTVWY\")\nSPECIAL_TOKENS = [\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"]\nVOCAB = SPECIAL_TOKENS + AMINO_ACIDS\ntoken2idx = {tok: idx for idx, tok in enumerate(VOCAB)}\nidx2token = {idx: tok for tok, idx in token2idx.items()}\nPAD_ID = token2idx[\"<PAD>\"]\nSOS_ID = token2idx[\"<SOS>\"]\nEOS_ID = token2idx[\"<EOS>\"]\nUNK_ID = token2idx[\"<UNK>\"]\nVOCAB_SIZE = len(VOCAB)\n\ndef tokenize(seq: str):\n    ids = [SOS_ID]\n    for ch in seq.strip():\n        ids.append(token2idx.get(ch, UNK_ID))\n    ids.append(EOS_ID)\n    return ids\n\n# -----------------------------\n# 2) Load Data\n# -----------------------------\ndf = pd.read_excel(DATA_PATH)\ndf.columns = [c.strip().replace(\" \", \"_\") for c in df.columns]\nassert SEQ_COL in df.columns and LABEL_COL in df.columns, \"Required columns missing\"\ndf = df.dropna(subset=[SEQ_COL, LABEL_COL]).reset_index(drop=True)\nprint(\"Total samples:\", len(df))\nprint(df[LABEL_COL].value_counts())\n\nsequences_orig = df[SEQ_COL].astype(str).tolist()\nlabels_text_orig = df[LABEL_COL].astype(str).tolist()\n\n# -----------------------------\n# 3) Augmentation (Sliding Windows)\n# -----------------------------\ndef sliding_windows(seq, center_len=WINDOW_LEN, shifts=AUG_SHIFTS):\n    L = len(seq)\n    windows = []\n    for s in shifts:\n        mid = L//2 + s\n        start = max(0, mid - center_len//2)\n        end = start + center_len\n        if end > L:\n            end = L\n            start = max(0, end - center_len)\n        w = seq[start:end]\n        if len(w) == center_len:\n            windows.append(w)\n    return windows\n\nif USE_AUGMENT:\n    sequences, labels_text = [], []\n    for seq, lab in zip(sequences_orig, labels_text_orig):\n        wins = sliding_windows(seq)\n        if len(wins) == 0:\n            s = seq[:WINDOW_LEN].ljust(WINDOW_LEN, 'A')  # Pad with 'A' if too short\n            sequences.append(s); labels_text.append(lab)\n        else:\n            for w in wins:\n                sequences.append(w); labels_text.append(lab)\n    print(\"After augmentation: total samples =\", len(sequences))\nelse:\n    sequences = sequences_orig\n    labels_text = labels_text_orig\n    print(\"No augmentation: total samples =\", len(sequences))\n\n# -----------------------------\n# 4) Label Encoding\n# -----------------------------\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform(labels_text)\nprint(\"Classes:\", label_encoder.classes_)\n\n# -----------------------------\n# 5) Custom ProteinLM (from previous)\n# -----------------------------\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, d_model, n_head, attn_dropout=0.1):\n        super().__init__()\n        assert d_model % n_head == 0\n        self.n_head = n_head\n        self.d_head = d_model // n_head\n        self.qkv = nn.Linear(d_model, 3 * d_model)\n        self.proj = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(attn_dropout)\n    def forward(self, x, mask=None):\n        B, T, C = x.size()\n        qkv = self.qkv(x)\n        q, k, v = qkv.chunk(3, dim=-1)\n        q = q.view(B, T, self.n_head, self.d_head).transpose(1, 2)\n        k = k.view(B, T, self.n_head, self.d_head).transpose(1, 2)\n        v = v.view(B, T, self.n_head, self.d_head).transpose(1, 2)\n        att = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head)\n        causal_mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)\n        att = att.masked_fill(causal_mask == 0, float(\"-inf\"))\n        if mask is not None:\n            mask2 = mask.unsqueeze(1).unsqueeze(2)\n            att = att.masked_fill(mask2 == 0, float(\"-inf\"))\n        att = F.softmax(att, dim=-1)\n        att = self.dropout(att)\n        out = torch.matmul(att, v)\n        out = out.transpose(1, 2).contiguous().view(B, T, C)\n        return self.proj(out)\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.GELU(),\n            nn.Linear(d_ff, d_model),\n            nn.Dropout(dropout),\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model, n_head, d_ff, dropout=0.1):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(d_model)\n        self.attn = CausalSelfAttention(d_model, n_head, attn_dropout=dropout)\n        self.ln2 = nn.LayerNorm(d_model)\n        self.ff = FeedForward(d_model, d_ff, dropout)\n    def forward(self, x, mask):\n        x = x + self.attn(self.ln1(x), mask)\n        x = x + self.ff(self.ln2(x))\n        return x\n\nclass ProteinLM(nn.Module):\n    def __init__(self, vocab_size=VOCAB_SIZE, d_model=256, nhead=8, num_layers=6, d_ff=1024, max_len=1024, dropout=0.1):\n        super().__init__()\n        self.token_emb = nn.Embedding(vocab_size, d_model)\n        self.pos_emb = nn.Embedding(max_len, d_model)\n        self.layers = nn.ModuleList([TransformerBlock(d_model, nhead, d_ff, dropout) for _ in range(num_layers)])\n        self.ln_f = nn.LayerNorm(d_model)\n        self.max_len = max_len\n        self.d_model = d_model\n\n    def forward(self, input_ids, attention_mask=None):\n        B, T = input_ids.size()\n        positions = torch.arange(0, T, device=input_ids.device).unsqueeze(0)\n        x = self.token_emb(input_ids) + self.pos_emb(positions)\n        for layer in self.layers:\n            x = layer(x, attention_mask)\n        x = self.ln_f(x)\n        return x  # (B, T, d_model)\n\n# Load pre-trained custom ProteinLM\nbase_model = ProteinLM()\nckpt = torch.load(CKPT_PATH, map_location=DEVICE)\nbase_model.load_state_dict(ckpt[\"model_state\"], strict=False)\nbase_model.to(DEVICE)\nbase_model.eval()\n\n# -----------------------------\n# 6) Embedding Extraction (Adapted for Custom Model)\n# -----------------------------\n@torch.no_grad()\ndef embed_batch(seqs, batch_size=BATCH_EMBED, max_length=MAX_TOKEN_LENGTH):\n    all_embs = []\n    base_model.eval()\n    for i in range(0, len(seqs), batch_size):\n        batch = seqs[i:i+batch_size]\n        # Tokenize batch\n        tokenized = []\n        attn_masks = []\n        for seq in batch:\n            ids = tokenize(seq)\n            if len(ids) > max_length:\n                ids = ids[:max_length-1] + [EOS_ID]\n            # Pad to max_length\n            padded_ids = ids + [PAD_ID] * (max_length - len(ids))\n            tokenized.append(torch.tensor(padded_ids, dtype=torch.long))\n            mask = [1 if tok != PAD_ID else 0 for tok in padded_ids]\n            attn_masks.append(torch.tensor(mask, dtype=torch.long))\n        \n        input_ids = torch.stack(tokenized).to(DEVICE)\n        attn_mask = torch.stack(attn_masks).to(DEVICE)\n        \n        # Forward through custom model\n        outputs = base_model(input_ids, attention_mask=attn_mask)\n        mean_emb = outputs.mean(dim=1)  # Mean pooling\n        all_embs.append(mean_emb.detach().cpu().float().numpy())\n    return np.vstack(all_embs)\n\nprint(\"Generating embeddings...\")\nt0 = time.time()\nembeddings = embed_batch(sequences, batch_size=BATCH_EMBED)\nprint(f\"Embeddings shape: {embeddings.shape}   (took {time.time()-t0:.1f}s)\")\n\n# -----------------------------\n# 7) PCA Dimensionality Reduction\n# -----------------------------\nif PCA_DIM is not None:\n    pca = PCA(n_components=min(PCA_DIM, embeddings.shape[1]), random_state=RANDOM_SEED)  # Avoid expansion if needed\n    embeddings = pca.fit_transform(embeddings)\n    print(\"PCA reduced to:\", embeddings.shape[1])\n\nX = torch.tensor(embeddings, dtype=torch.float32)\ny = torch.tensor(labels, dtype=torch.long)\n\n# -----------------------------\n# 8) Train/Test Split\n# -----------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X.numpy(), y.numpy(), test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y.numpy()\n)\nX_train = torch.tensor(X_train, dtype=torch.float32)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.long)\ny_test = torch.tensor(y_test, dtype=torch.long)\n\n# -----------------------------\n# 9) DataLoaders with Weighted Sampling\n# -----------------------------\ntrain_ds = TensorDataset(X_train, y_train)\ntest_ds = TensorDataset(X_test, y_test)\n\nclass_counts = torch.bincount(y_train)\ninv_freq = 1.0 / (class_counts.float() + 1e-9)\nclass_weights = inv_freq / inv_freq.sum()\n\nif USE_SAMPLER:\n    sample_weights = class_weights[y_train].cpu().numpy()\n    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_TRAIN, sampler=sampler, drop_last=True)\nelse:\n    train_loader = DataLoader(train_ds, batch_size=BATCH_TRAIN, shuffle=True, drop_last=True)\n\ntest_loader = DataLoader(test_ds, batch_size=BATCH_TRAIN, shuffle=False, drop_last=False)\n\n# -----------------------------\n# 10) Advanced MLP Classifier\n# -----------------------------\ninput_dim = X_train.shape[1]\nnum_classes = len(label_encoder.classes_)\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden1=HIDDEN_DIM_1, hidden2=HIDDEN_DIM_2, num_classes=num_classes, dropout=DROPOUT):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden1),\n            nn.BatchNorm1d(hidden1),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden1, hidden2),\n            nn.BatchNorm1d(hidden2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden2, num_classes)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nmodel_mlp = MLP(input_dim).to(DEVICE)\n\n# -----------------------------\n# 11) Training Setup\n# -----------------------------\nloss_fn = nn.CrossEntropyLoss(weight=class_weights.to(DEVICE))\noptimizer = AdamW(model_mlp.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n                                                       factor=LR_PLATEAU_FACTOR,\n                                                       patience=LR_PLATEAU_PATIENCE,\n                                                       verbose=True)\n\nbest_val_loss = float(\"inf\")\npatience_counter = 0\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss = 0.0\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n            out = model(xb)\n            loss = loss_fn(out, yb)\n            total_loss += loss.item() * xb.size(0)\n            preds = torch.argmax(out, dim=1)\n            all_preds.append(preds.cpu().numpy())\n            all_labels.append(yb.cpu().numpy())\n    avg_loss = total_loss / len(loader.dataset)\n    all_preds = np.concatenate(all_preds)\n    all_labels = np.concatenate(all_labels)\n    acc = (all_preds == all_labels).mean()\n    return avg_loss, acc, all_preds, all_labels\n\n# -----------------------------\n# 12) Training Loop with Early Stopping & LR Scheduling\n# -----------------------------\nprint(\"Training...\")\nfor epoch in range(1, NUM_EPOCHS+1):\n    model_mlp.train()\n    epoch_loss = 0.0\n    for xb, yb in train_loader:\n        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n        optimizer.zero_grad()\n        out = model_mlp(xb)\n        loss = loss_fn(out, yb)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item() * xb.size(0)\n    avg_train_loss = epoch_loss / len(train_loader.dataset)\n    val_loss, val_acc, _, _ = evaluate(model_mlp, test_loader)\n    print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n    scheduler.step(val_loss)\n    if val_loss < best_val_loss - 1e-6:\n        best_val_loss = val_loss\n        patience_counter = 0\n        torch.save(model_mlp.state_dict(), \"best_mlp_custom_adv.pth\")\n        print(\"  ✅ Saved best model.\")\n    else:\n        patience_counter += 1\n        if patience_counter >= PATIENCE:\n            print(\"⏹ Early stopping.\")\n            break\n\n# -----------------------------\n# 13) Final Evaluation\n# -----------------------------\nmodel_mlp.load_state_dict(torch.load(\"best_mlp_custom_adv.pth\"))\nval_loss, val_acc, val_preds, val_labels = evaluate(model_mlp, test_loader)\nprint(f\"Final Val Acc: {val_acc*100:.2f}%\")\nprint(classification_report(val_labels, val_preds, target_names=label_encoder.classes_))\n\n# -----------------------------\n# 14) Save for Inference (e.g., Flask) - WITH EXPLICIT PATHS & VERIFICATION\n# -----------------------------\nOUTPUT_DIR = \"/kaggle/working\"\ntorch.save(model_mlp.state_dict(), os.path.join(OUTPUT_DIR, \"best_mlp_custom_adv.pth\"))\njoblib.dump(label_encoder, os.path.join(OUTPUT_DIR, \"label_encoder.pkl\"))\nif PCA_DIM is not None:\n    joblib.dump(pca, os.path.join(OUTPUT_DIR, \"pca_model.pkl\"))\n\n# Verify saves\nprint(\"\\nSaved files:\")\nfor f in [\"best_mlp_custom_adv.pth\", \"label_encoder.pkl\", \"pca_model.pkl\"]:\n    path = os.path.join(OUTPUT_DIR, f)\n    exists = os.path.exists(path)\n    size = os.path.getsize(path) if exists else 0\n    print(f\" - {f}: {'✅ Exists' if exists else '❌ Missing'} ({size} bytes)\")\n\nprint(\"\\n📂 Files saved for inference:\")\nprint(\" - best_mlp_custom_adv.pth (MLP weights)\")\nprint(\" - label_encoder.pkl (class mapping)\")\nif PCA_DIM is not None:\n    print(\" - pca_model.pkl (PCA reducer)\")\n\nprint(\"\\nYou can now load these for predictions on new sequences.\")\n\n# Final verification: List all files in working dir\nprint(\"\\nAll files in /kaggle/working/:\")\n!ls -la /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T17:00:16.897881Z","iopub.execute_input":"2025-10-26T17:00:16.898126Z","iopub.status.idle":"2025-10-26T17:00:55.883199Z","shell.execute_reply.started":"2025-10-26T17:00:16.898107Z","shell.execute_reply":"2025-10-26T17:00:55.881649Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nTotal samples: 891\nClinical_Significance\nPathogenic           263\nLikely Benign        217\nLikely Pathogenic    217\nBenign               194\nName: count, dtype: int64\nAfter augmentation: total samples = 2671\nClasses: ['Benign' 'Likely Benign' 'Likely Pathogenic' 'Pathogenic']\nGenerating embeddings...\nEmbeddings shape: (2671, 256)   (took 13.5s)\nPCA reduced to: 256\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Training...\nEpoch 1 | Train Loss: 1.3915 | Val Loss: 1.2726 | Val Acc: 45.42%\n  ✅ Saved best model.\nEpoch 2 | Train Loss: 1.2517 | Val Loss: 1.1697 | Val Acc: 53.83%\n  ✅ Saved best model.\nEpoch 3 | Train Loss: 1.1732 | Val Loss: 1.0893 | Val Acc: 59.63%\n  ✅ Saved best model.\nEpoch 4 | Train Loss: 1.0897 | Val Loss: 1.0288 | Val Acc: 62.99%\n  ✅ Saved best model.\nEpoch 5 | Train Loss: 1.0248 | Val Loss: 0.9642 | Val Acc: 65.98%\n  ✅ Saved best model.\nEpoch 6 | Train Loss: 0.9699 | Val Loss: 0.9092 | Val Acc: 68.22%\n  ✅ Saved best model.\nEpoch 7 | Train Loss: 0.9031 | Val Loss: 0.8583 | Val Acc: 71.03%\n  ✅ Saved best model.\nEpoch 8 | Train Loss: 0.8529 | Val Loss: 0.8012 | Val Acc: 73.27%\n  ✅ Saved best model.\nEpoch 9 | Train Loss: 0.8036 | Val Loss: 0.7586 | Val Acc: 76.07%\n  ✅ Saved best model.\nEpoch 10 | Train Loss: 0.7817 | Val Loss: 0.7087 | Val Acc: 77.76%\n  ✅ Saved best model.\nEpoch 11 | Train Loss: 0.7100 | Val Loss: 0.6696 | Val Acc: 79.25%\n  ✅ Saved best model.\nEpoch 12 | Train Loss: 0.6597 | Val Loss: 0.6363 | Val Acc: 79.44%\n  ✅ Saved best model.\nEpoch 13 | Train Loss: 0.6297 | Val Loss: 0.5953 | Val Acc: 80.19%\n  ✅ Saved best model.\nEpoch 14 | Train Loss: 0.5987 | Val Loss: 0.5632 | Val Acc: 82.24%\n  ✅ Saved best model.\nEpoch 15 | Train Loss: 0.5519 | Val Loss: 0.5294 | Val Acc: 82.80%\n  ✅ Saved best model.\nEpoch 16 | Train Loss: 0.5197 | Val Loss: 0.4974 | Val Acc: 84.11%\n  ✅ Saved best model.\nEpoch 17 | Train Loss: 0.4975 | Val Loss: 0.4676 | Val Acc: 85.79%\n  ✅ Saved best model.\nEpoch 18 | Train Loss: 0.4749 | Val Loss: 0.4366 | Val Acc: 86.73%\n  ✅ Saved best model.\nEpoch 19 | Train Loss: 0.4274 | Val Loss: 0.4158 | Val Acc: 85.79%\n  ✅ Saved best model.\nEpoch 20 | Train Loss: 0.4135 | Val Loss: 0.3908 | Val Acc: 88.04%\n  ✅ Saved best model.\nEpoch 21 | Train Loss: 0.3869 | Val Loss: 0.3623 | Val Acc: 89.91%\n  ✅ Saved best model.\nEpoch 22 | Train Loss: 0.3498 | Val Loss: 0.3484 | Val Acc: 89.53%\n  ✅ Saved best model.\nEpoch 23 | Train Loss: 0.3471 | Val Loss: 0.3286 | Val Acc: 90.09%\n  ✅ Saved best model.\nEpoch 24 | Train Loss: 0.3223 | Val Loss: 0.3123 | Val Acc: 89.35%\n  ✅ Saved best model.\nEpoch 25 | Train Loss: 0.3193 | Val Loss: 0.2959 | Val Acc: 90.47%\n  ✅ Saved best model.\nEpoch 26 | Train Loss: 0.2964 | Val Loss: 0.2731 | Val Acc: 90.47%\n  ✅ Saved best model.\nEpoch 27 | Train Loss: 0.2856 | Val Loss: 0.2674 | Val Acc: 90.47%\n  ✅ Saved best model.\nEpoch 28 | Train Loss: 0.2690 | Val Loss: 0.2461 | Val Acc: 91.21%\n  ✅ Saved best model.\nEpoch 29 | Train Loss: 0.2615 | Val Loss: 0.2378 | Val Acc: 91.40%\n  ✅ Saved best model.\nEpoch 30 | Train Loss: 0.2350 | Val Loss: 0.2194 | Val Acc: 92.71%\n  ✅ Saved best model.\nEpoch 31 | Train Loss: 0.2408 | Val Loss: 0.2131 | Val Acc: 92.15%\n  ✅ Saved best model.\nEpoch 32 | Train Loss: 0.2360 | Val Loss: 0.2005 | Val Acc: 93.64%\n  ✅ Saved best model.\nEpoch 33 | Train Loss: 0.2341 | Val Loss: 0.1920 | Val Acc: 93.83%\n  ✅ Saved best model.\nEpoch 34 | Train Loss: 0.2192 | Val Loss: 0.1930 | Val Acc: 93.64%\nEpoch 35 | Train Loss: 0.2113 | Val Loss: 0.1846 | Val Acc: 94.02%\n  ✅ Saved best model.\nEpoch 36 | Train Loss: 0.1858 | Val Loss: 0.1734 | Val Acc: 94.02%\n  ✅ Saved best model.\nEpoch 37 | Train Loss: 0.1780 | Val Loss: 0.1680 | Val Acc: 94.02%\n  ✅ Saved best model.\nEpoch 38 | Train Loss: 0.1873 | Val Loss: 0.1633 | Val Acc: 94.58%\n  ✅ Saved best model.\nEpoch 39 | Train Loss: 0.2065 | Val Loss: 0.1590 | Val Acc: 94.02%\n  ✅ Saved best model.\nEpoch 40 | Train Loss: 0.1810 | Val Loss: 0.1562 | Val Acc: 94.39%\n  ✅ Saved best model.\nEpoch 41 | Train Loss: 0.1788 | Val Loss: 0.1500 | Val Acc: 95.14%\n  ✅ Saved best model.\nEpoch 42 | Train Loss: 0.1788 | Val Loss: 0.1439 | Val Acc: 95.33%\n  ✅ Saved best model.\nEpoch 43 | Train Loss: 0.1695 | Val Loss: 0.1373 | Val Acc: 95.51%\n  ✅ Saved best model.\nEpoch 44 | Train Loss: 0.1596 | Val Loss: 0.1271 | Val Acc: 96.26%\n  ✅ Saved best model.\nEpoch 45 | Train Loss: 0.1561 | Val Loss: 0.1363 | Val Acc: 95.33%\nEpoch 46 | Train Loss: 0.1606 | Val Loss: 0.1352 | Val Acc: 94.77%\nEpoch 47 | Train Loss: 0.1552 | Val Loss: 0.1253 | Val Acc: 95.33%\n  ✅ Saved best model.\nEpoch 48 | Train Loss: 0.1592 | Val Loss: 0.1242 | Val Acc: 96.07%\n  ✅ Saved best model.\nEpoch 49 | Train Loss: 0.1311 | Val Loss: 0.1264 | Val Acc: 95.89%\nEpoch 50 | Train Loss: 0.1665 | Val Loss: 0.1113 | Val Acc: 96.64%\n  ✅ Saved best model.\nEpoch 51 | Train Loss: 0.1375 | Val Loss: 0.1137 | Val Acc: 96.45%\nEpoch 52 | Train Loss: 0.1395 | Val Loss: 0.1126 | Val Acc: 96.45%\nEpoch 53 | Train Loss: 0.1459 | Val Loss: 0.1149 | Val Acc: 96.26%\nEpoch 54 | Train Loss: 0.1388 | Val Loss: 0.1142 | Val Acc: 95.89%\nEpoch 55 | Train Loss: 0.1172 | Val Loss: 0.1168 | Val Acc: 95.70%\nEpoch 56 | Train Loss: 0.1237 | Val Loss: 0.1133 | Val Acc: 96.07%\nEpoch 57 | Train Loss: 0.1302 | Val Loss: 0.1114 | Val Acc: 96.26%\nEpoch 58 | Train Loss: 0.1202 | Val Loss: 0.1112 | Val Acc: 96.26%\n  ✅ Saved best model.\nEpoch 59 | Train Loss: 0.1252 | Val Loss: 0.1146 | Val Acc: 96.45%\nEpoch 60 | Train Loss: 0.1240 | Val Loss: 0.1147 | Val Acc: 95.70%\nEpoch 61 | Train Loss: 0.1283 | Val Loss: 0.1089 | Val Acc: 96.82%\n  ✅ Saved best model.\nEpoch 62 | Train Loss: 0.1067 | Val Loss: 0.1082 | Val Acc: 96.26%\n  ✅ Saved best model.\nEpoch 63 | Train Loss: 0.1091 | Val Loss: 0.1051 | Val Acc: 96.07%\n  ✅ Saved best model.\nEpoch 64 | Train Loss: 0.1169 | Val Loss: 0.1038 | Val Acc: 96.07%\n  ✅ Saved best model.\nEpoch 65 | Train Loss: 0.1224 | Val Loss: 0.1038 | Val Acc: 96.26%\n  ✅ Saved best model.\nEpoch 66 | Train Loss: 0.0947 | Val Loss: 0.1033 | Val Acc: 96.26%\n  ✅ Saved best model.\nEpoch 67 | Train Loss: 0.0991 | Val Loss: 0.0998 | Val Acc: 96.45%\n  ✅ Saved best model.\nEpoch 68 | Train Loss: 0.1145 | Val Loss: 0.0979 | Val Acc: 96.64%\n  ✅ Saved best model.\nEpoch 69 | Train Loss: 0.1083 | Val Loss: 0.0986 | Val Acc: 96.26%\nEpoch 70 | Train Loss: 0.1153 | Val Loss: 0.0992 | Val Acc: 96.45%\nEpoch 71 | Train Loss: 0.1076 | Val Loss: 0.0964 | Val Acc: 96.82%\n  ✅ Saved best model.\nEpoch 72 | Train Loss: 0.1131 | Val Loss: 0.0966 | Val Acc: 96.82%\nEpoch 73 | Train Loss: 0.1007 | Val Loss: 0.0992 | Val Acc: 96.45%\nEpoch 74 | Train Loss: 0.1001 | Val Loss: 0.0989 | Val Acc: 96.07%\nEpoch 75 | Train Loss: 0.0878 | Val Loss: 0.1029 | Val Acc: 96.45%\nEpoch 76 | Train Loss: 0.0957 | Val Loss: 0.0979 | Val Acc: 96.45%\nEpoch 77 | Train Loss: 0.1093 | Val Loss: 0.1023 | Val Acc: 96.07%\nEpoch 78 | Train Loss: 0.1141 | Val Loss: 0.0996 | Val Acc: 96.26%\nEpoch 79 | Train Loss: 0.1068 | Val Loss: 0.1005 | Val Acc: 96.26%\nEpoch 80 | Train Loss: 0.0972 | Val Loss: 0.1043 | Val Acc: 96.07%\nEpoch 81 | Train Loss: 0.1092 | Val Loss: 0.1029 | Val Acc: 96.07%\nEpoch 82 | Train Loss: 0.1029 | Val Loss: 0.1040 | Val Acc: 96.26%\nEpoch 83 | Train Loss: 0.1149 | Val Loss: 0.1062 | Val Acc: 96.07%\nEpoch 84 | Train Loss: 0.1046 | Val Loss: 0.1059 | Val Acc: 96.26%\nEpoch 85 | Train Loss: 0.1134 | Val Loss: 0.1039 | Val Acc: 96.07%\nEpoch 86 | Train Loss: 0.1036 | Val Loss: 0.1029 | Val Acc: 95.89%\nEpoch 87 | Train Loss: 0.1057 | Val Loss: 0.1016 | Val Acc: 96.07%\nEpoch 88 | Train Loss: 0.1016 | Val Loss: 0.1035 | Val Acc: 96.07%\nEpoch 89 | Train Loss: 0.0877 | Val Loss: 0.1043 | Val Acc: 96.07%\nEpoch 90 | Train Loss: 0.0890 | Val Loss: 0.1041 | Val Acc: 96.07%\nEpoch 91 | Train Loss: 0.1013 | Val Loss: 0.1038 | Val Acc: 95.89%\nEpoch 92 | Train Loss: 0.1008 | Val Loss: 0.1029 | Val Acc: 96.07%\nEpoch 93 | Train Loss: 0.0961 | Val Loss: 0.1039 | Val Acc: 96.07%\nEpoch 94 | Train Loss: 0.0969 | Val Loss: 0.1039 | Val Acc: 96.26%\nEpoch 95 | Train Loss: 0.0916 | Val Loss: 0.1035 | Val Acc: 96.26%\nEpoch 96 | Train Loss: 0.0974 | Val Loss: 0.1018 | Val Acc: 96.07%\n⏹ Early stopping.\nFinal Val Acc: 96.82%\n                   precision    recall  f1-score   support\n\n           Benign       0.96      0.96      0.96       117\n    Likely Benign       0.98      0.98      0.98       130\nLikely Pathogenic       0.95      0.98      0.96       130\n       Pathogenic       0.99      0.96      0.97       158\n\n         accuracy                           0.97       535\n        macro avg       0.97      0.97      0.97       535\n     weighted avg       0.97      0.97      0.97       535\n\n\nSaved files:\n - best_mlp_custom_adv.pth: ✅ Exists (1073814 bytes)\n - label_encoder.pkl: ✅ Exists (599 bytes)\n - pca_model.pkl: ✅ Exists (267127 bytes)\n\n📂 Files saved for inference:\n - best_mlp_custom_adv.pth (MLP weights)\n - label_encoder.pkl (class mapping)\n - pca_model.pkl (PCA reducer)\n\nYou can now load these for predictions on new sequences.\n\nAll files in /kaggle/working/:\ntotal 1332\ndrwxr-xr-x 3 root root    4096 Oct 26 17:00 .\ndrwxr-xr-x 5 root root    4096 Oct 26 16:59 ..\n-rw-r--r-- 1 root root 1073814 Oct 26 17:00 best_mlp_custom_adv.pth\n-rw-r--r-- 1 root root     599 Oct 26 17:00 label_encoder.pkl\n-rw-r--r-- 1 root root  267127 Oct 26 17:00 pca_model.pkl\ndrwxr-xr-x 2 root root    4096 Oct 26 17:00 .virtual_documents\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}